## Build a data pipeline with Dagster
---

### Introduction
The objective of this practice is to get a data pipeline up and running quickly using **Dagster** - an orchestrator that's designed for developing and maintaining data assets. The workflow includes:
1. **Task 1**: Extracting data (*parquet files*) from online source (website) to a local machine.
2. **Task 2**: Transforming this data, using Pandas library.
3. **Task 3**: Loading transformed data into a local PostgreSQL database.

The above workflow will be developed locally with **Dagster**. Further info on Dagster can be found in the following: https://dagster.io/

The dataset used in this practice is TLC Trip Record Data for green taxi (format: *parquet file*). For a quick experiment, the dataset only consists of the taxi trips in the two months of 2023.

The Dataset and Data Dictionary used in this practice can be found and downloaded in the folllowing:
1. Dataset: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page
2. Data Dictionary: https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf

Tech stack:
- Python 3.10
- PostgreSQL 10
- dagster (1.4.12)

---
### Workflow Overview in this practice

  ![workflow](https://github.com/DoThNg/Data-Engineering-Projects/blob/main/4_ETL_Dagster/docs/dagster_project_workflow.png)

---

### Steps to run the data pipeline with Dagster:
**Step 1:** Set up the virtual environment 
- Run command: 'python -m venv {virtualenv name}' 

**Step 2:** Create a Dagster project
- Run command: 'dagster project scaffold --name etl-project'
- Go to setup.py in the newly created Dagster project folder and add python libraries used in this practice (Reference [setup.py](https://github.com/DoThNg/Data-Engineering-Projects/blob/main/4_ETL_Dagster/etl-project/setup.py)) . 

**Step 3:** Set up a local PostgreSQL database (PostgreSQL 10 is used in this practice)

**Step 4:** Store credentials to create a database connection in a .env file (Reference: [env-template](https://github.com/DoThNg/Data-Engineering-Projects/blob/main/4_ETL_Dagster/env-template)) and save this file in folder where the virtual env is created. 

**Step 5:** Create assets for the data pipeline

- The following files are used for data pipeline with Dagster (These files are placed in the **~/elt-project/etl_project directory**):

 - [etl-project/etl_project/sql.py](https://github.com/DoThNg/Data-Engineering-Projects/blob/main/4_ETL_Dagster/etl-project/etl_project/sql.py)
 - Task 1 [Extracting Data]: [etl-project/etl_project/extract_data_from_source.py](https://github.com/DoThNg/Data-Engineering-Projects/blob/main/4_ETL_Dagster/etl-project/etl_project/extract_taxi_data.py)
 - Task 2 [Transforming Data]: [etl-project/etl_project/transform_taxi_data.py](https://github.com/DoThNg/Data-Engineering-Projects/blob/main/4_ETL_Dagster/etl-project/etl_project/transform_taxi_data.py)
 - Task 3 [Loading Data]: [etl-project/etl_project/load_taxi_data.py](https://github.com/DoThNg/Data-Engineering-Projects/blob/main/4_ETL_Dagster/etl-project/etl_project/load_data_to_postgresql.py)

- Each task above will represent an asset in the [asset.py](https://github.com/DoThNg/Data-Engineering-Projects/blob/main/4_ETL_Dagster/etl-project/etl_project/assets.py) - this file is automatically generated after creating a Dagster project (**Note:** all codes for each task can be put into asset.py; however for a clarity, each task is written as a python module and then imported in the asset.py file). The folder structure (**etl_project**) is as follows:

  ![etl_project](https://github.com/DoThNg/Data-Engineering-Projects/blob/main/4_ETL_Dagster/docs/etl_project.png)

The overall view of data pipeline is as follows:

  ![data pipeline](https://github.com/DoThNg/Data-Engineering-Projects/blob/main/4_ETL_Dagster/docs/dagster_data_pipeline.png)

**Step 6:** Run the data pipeline on Dagster web server
- Run command: 'cd etl-project' and then 'pip install -e ".[dev]"' (This will install python library dependencies)
- Run command: 'dagster dev' (This will launch the Dagster webserver/UI at localhost:3000 in the browser)

**Note**: The above commands should be run in a Python environment (in **etl-project** directory) where the dagster and dagster-webserver packages are installed (Further reference for local deployment: [docs](https://docs.dagster.io/guides/running-dagster-locally)) 
