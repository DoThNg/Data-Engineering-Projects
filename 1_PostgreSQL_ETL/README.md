## Quick ELT Practice with PostgreSQL and dbt
---

### Introduction
This is just a simple practice to perform ELT process, including:
1. Task 1: Loading files (*.parquet*) into a local PostgreSQL database.
2. Task 2: Transforming dataset with dbt. The transformed dataset can be used to build BI Dashboards later.

The dataset used in this practice include:
1. TLC Trip Record Data for green taxi (format: *parquet files*).
2. Taxi Zone Maps and Lookup data (format: *CSV file*)

The Dataset and Data Dictionary used in this practice can be found and downloaded in the folllowing: 
1. Dataset: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page
2. Data Dictionary: https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf

Note: In this practice, dataset consists of taxi trips for the first 5 months of 2023, which include 5 parquet files:
- green_tripdata_2023-01.parquet
- green_tripdata_2023-02.parquet  
- green_tripdata_2023-03.parquet
- green_tripdata_2023-04.parquet
- green_tripdata_2023-05.parquet

Dataset - Retrieved August 20, 2023, from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page

Tech stack:
- Python 3.10
- dbt-core (1.6.0)
- dbt-postgres (1.6.0)
- PostgreSQL 10

---
### Workflow Overview in this practice

  ![workflow](https://github.com/DoThNg/Data-Engineering-Projects/blob/main/1_PostgreSQL_ETL/docs/elt_workflow.png)

---

### Steps to run this ELT process:
**Step 1:** Set up the virtual environment
- Create a folder for this practice.
- Run command: *python -m venv {virtualenv name}* in this folder's directory.
- Create a folder named *'dataset'* where the virtual env is created. 
- Save the data files (*parquet files*) in this folder.
- Save following files [setup_tbl.py](https://github.com/DoThNg/Data-Engineering-Projects/blob/main/1_PostgreSQL_ETL/setup_tbl.py), [sql.py](https://github.com/DoThNg/Data-Engineering-Projects/blob/main/1_PostgreSQL_ETL/sql.py), and [elt.py](https://github.com/DoThNg/Data-Engineering-Projects/blob/main/1_PostgreSQL_ETL/elt.py) in the folder created for this practice.  

**Step 2:** Run command: `pip install -r requirements.txt` (This will install all relevant python packages for this practice)

**Step 3:** Set up a local PostgreSQL database (PostgreSQL 10 is used in this practice)

**Step 4:** Store credentials to create a database connection in a `.env` file and store this file in the folder created for this practice (Reference: [env-template](https://github.com/DoThNg/Data-Engineering-Projects/blob/main/1_PostgreSQL_ETL/env-template))

**Step 5:** Run command: `python setup_tbl.py` (This will create a data table in the newly created database)

**Step 6:** Run command: `python elt.py` (This step is about loading data (*parquet files*) to the newly created data table)

**Step 7:** Using dbt to transform the loaded data. 

For a quick setup of dbt project, reference this [docs](https://docs.getdbt.com/quickstarts/manual-install?step=2). 

Run the following commands to set up dbt project: 
- Project setup: dbt init analytics (In this Practice, Project Name is **analytics**)
- Connect to PostgreSQL: 
  - Open YAML file (*profiles.yml*) created in the **~/.dbt/** directory (This file is automatically generated after running: dbt init analytics). 
  - Provide info (e.g., host, port, user, etc.) related to PostgreSQL database as indicated in the following image. (Note: For this practice, set 'dev' to 'target' (target: dev)) 

  ![YAML file](https://github.com/DoThNg/Data-Engineering-Projects/blob/main/1_PostgreSQL_ETL/docs/dbt_yaml_file.png)


Note: Running the dbt project from the command line in the current dbt project directory.

- Test connection to the local PostgreSQL database: dbt debug 
- Create a sub-folder 'seeds' in project folder and add the file to the seeds directory, with a .csv file extension (In this practice, *'taxi_zone_lookup.csv'* file will be used)
- Create sub-folders *'staging'* and *'mart'* in folder *'models'*. 
- After this setup, the overall project structure is as follows: 

  ![dbt project structure](https://github.com/DoThNg/Data-Engineering-Projects/blob/main/1_PostgreSQL_ETL/docs/dbt_project_structure.png)


- Configurations in *dbt_project.yml* file: Open YAML file (dbt_project.yml) created in the dbt project's directory (This file is also automatically generated after running: dbt init analytics). Set up relevant project configurations used in this practice (Reference: [dbt_project.yml](https://github.com/DoThNg/Data-Engineering-Projects/blob/main/1_PostgreSQL_ETL/analytics/dbt_project.yml))
- Run command: dbt seed (a new table will be created in the warehouse in dbt project's target schema, named *taxi_zone_lookup*)
- Save *.sql* files in sub-folders *'mart'* and *'staging'* 
Reference:
   - *.sql* files in sub-folder *'mart'*: [models/mart](https://github.com/DoThNg/Data-Engineering-Projects/tree/main/1_PostgreSQL_ETL/analytics/models/mart)
   - *.sql* files in sub-folder *'staging'*: [models/staging](https://github.com/DoThNg/Data-Engineering-Projects/tree/main/1_PostgreSQL_ETL/analytics/models/staging)

- Run command: `dbt run`

---

### dbt - Directed Acyclic Graph (DAG) 
The DAG of how data flows after transformation step with dbt is as follows:

  ![dag](https://github.com/DoThNg/Data-Engineering-Projects/blob/main/1_PostgreSQL_ETL/docs/dbt-dag.png)


